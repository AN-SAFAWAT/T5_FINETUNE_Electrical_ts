{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8277033,"sourceType":"datasetVersion","datasetId":4915004}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T03:25:48.195124Z","iopub.execute_input":"2024-05-01T03:25:48.196057Z","iopub.status.idle":"2024-05-01T03:25:49.503161Z","shell.execute_reply.started":"2024-05-01T03:25:48.196013Z","shell.execute_reply":"2024-05-01T03:25:49.502023Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/t5finetuned-for-elec/final_model/config.json\n/kaggle/input/t5finetuned-for-elec/final_model/spiece.model\n/kaggle/input/t5finetuned-for-elec/final_model/training_args.bin\n/kaggle/input/t5finetuned-for-elec/final_model/tokenizer_config.json\n/kaggle/input/t5finetuned-for-elec/final_model/model.safetensors\n/kaggle/input/t5finetuned-for-elec/final_model/special_tokens_map.json\n/kaggle/input/t5finetuned-for-elec/final_model/added_tokens.json\n/kaggle/input/t5finetuned-for-elec/final_model/generation_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T03:25:49.504848Z","iopub.execute_input":"2024-05-01T03:25:49.505432Z","iopub.status.idle":"2024-05-01T03:26:09.796247Z","shell.execute_reply.started":"2024-05-01T03:25:49.505391Z","shell.execute_reply":"2024-05-01T03:26:09.795102Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained('/kaggle/input/t5finetuned-for-elec/final_model')\ntokenizer = T5Tokenizer.from_pretrained('/kaggle/input/t5finetuned-for-elec/final_model')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T03:26:13.841415Z","iopub.execute_input":"2024-05-01T03:26:13.841799Z","iopub.status.idle":"2024-05-01T03:26:21.069539Z","shell.execute_reply.started":"2024-05-01T03:26:13.841766Z","shell.execute_reply":"2024-05-01T03:26:21.068770Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_text(prompt):\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    output_ids = model.generate(input_ids)\n    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n# Example usage\nprompt = \"root cause: Example root cause description -> action:\"\nprint(generate_text(prompt))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T03:26:24.867532Z","iopub.execute_input":"2024-05-01T03:26:24.867950Z","iopub.status.idle":"2024-05-01T03:26:25.621454Z","shell.execute_reply.started":"2024-05-01T03:26:24.867918Z","shell.execute_reply":"2024-05-01T03:26:25.620355Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"We just checked all HMD signal related with the corresponding code.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}